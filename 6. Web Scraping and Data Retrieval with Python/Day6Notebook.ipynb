{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Day 6: Web Scraping and Data Processing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be introducing Python as a multipurpose tool for fetching, parsing, aggregating, and exporting data! We will be drawing upon the BeautifulSoup library in particular and using some ideas from [\"Intro to Beautiful Soup\" by Jeri Wieringa](https://programminghistorian.org/en/lessons/intro-to-beautiful-soup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get going today, please complete the [Python Syntax tutorial at Codecademy](https://www.codecademy.com/courses/learn-python/lessons/python-syntax). \n",
    "\n",
    "Note that we will be using Python 3 in this notebook, not Python 2. As the Codecademy tutorial mentions, one critical change between 2 and 3 is that Python 3 uses the following syntax for print statements: print(\"Statement\")\n",
    "\n",
    "Click into the code box below for an example of printing in Python 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this code, click into this box and then click the Run button at the top of the screen\n",
    "\n",
    "researchQ = \"???\"\n",
    "print(\"A key question in my current research project is: \" + researchQ)\n",
    "\n",
    "# Now try assigning a different value to researchQ. Run the code again to see a different answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be adding a few concepts beyond the Codecademy tutorial. One important technique is the idea of **looping** or **iteration**. Sometimes we may wish to run the same line of code several times, but perhaps manipulating a different value each time.\n",
    "\n",
    "For instance, consider the following variable assignemnt for *methods*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The methods variable stores a list of three separate strings\n",
    "\n",
    "methods = [\"text analysis\", \"web mapping\", \"data cleaning\", \"lots of Googling\", \"inventing new methods entirely\"]\n",
    "\n",
    "# Click into this box and select RUN to assign the values to the methods variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python uses several data types to store data in variables. We will often want to collect our data into a single collection of values. \n",
    "\n",
    "A **list** is an ordered collection of values. We can create a list by using brackets around a collection of items, each of which is separated by a comma.\n",
    "\n",
    "In this case, we created a list of **strings**. Strings are sequences of alphanumeric characters. We create a string by using quotation marks (single-quotes, double-quotes, or triple-quotes all work, as long as we use the same type at the beginning and the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we print out each of these strings one by one? One approach is to tell Python to print multiple times\n",
    "Selecting a differnt item from the list using the \"index\", or number corresponding to the location of the item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Digital humanities involves \" + methods[0])\n",
    "print(\"Digital humanities involves \" + methods[1])\n",
    "print(\"Digital humanities involves \" + methods[2])\n",
    "print(\"Digital humanities involves \" + methods[3])\n",
    "print(\"Digital humanities involves \" + methods[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works, but is repetitive to write. (In programming, we try to follow the DRY princniple: Don't Repeat Yourself!)\n",
    "\n",
    "Instead, let's loop over the numbers 0, 1, 2, 3, 4 using a for loop and a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in [0, 1, 2, 3, 4]:\n",
    "    print(\"Digital humanities involves \" + methods[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses far less code and produces the same result.\n",
    "\n",
    "Sometimes, we would rather ignore the idea of the index entirely. Python lets us ignore indexes if we choose and loop over a list directly using the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    print(\"Digital humanities involves \" + method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of looping is very powerful, especially when we don't know ahead of time what values we will use.\n",
    "\n",
    "In web scraping, we often know a few values like the URL of the target site, and perhaps the location of important information on that site. But we will also discover information as we go, and may want to loop over these items to extract information (such as metadata, content, and links to further valuable information elsewhere). This is where looping helps us out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data that we haven't prepared ourselves, we often face a set of challenges in being able to use that data in a helpful fashion. In **web scraping** with Python, we first must fetch the data from our website. In most cases, that data will exist in HTML format, but it could be another format (like XML or others). \n",
    "\n",
    "Once our script has fetched the web data into working memory, we will likely want to transform a few important features of it into data structures Python understands (like strings, lists, and other variables). When scraping a blog, for example, we may wish to keep track of individual posts, and collect data like the title, date, post summary text, and link to the full text.\n",
    "\n",
    "Sometimes, we will want to perform additional transformations on our data, such as counting or aggregating certain categories of data, or looping over additional pages to fetch more web content.\n",
    "\n",
    "Finally, we'll want to output our data into a format that is helpful to us. In this example, we will output our data as a table using the **comma-separated values (CSV) format**. Another common output type is **JavaScript Object Notation (JSON)**.\n",
    "\n",
    "In short, our steps are:\n",
    "* Fetch data from a website\n",
    "* Transform the data (give it structure, keep track of important data and metadata, count things)\n",
    "* Output the data into a helpful format (CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fetch data from a website using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start scraping web data with Python, we first have to add some extra functinoality to our code.\n",
    "\n",
    "Python comes ready-built with several different functions, such as print(). However, we will often need to include extra functionality depending on our goal. Python collects this extra functionality into distinct collections of code called **libraries**. Some libraries are already installed alongside Python, and others need to be downloaded from the Internet (on Azure, Microsoft has already pre-downloaded most libraries you would need onto their servers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BeautifulSoup** is a web scraping library for Python. Its strength is giving Python the ability to understand the structure of a website, which often includes many different types of **tags** that are organized in a hierarchical fashion. BeautifulSoup lets you find all tags that match your search conditions and extract attributes and text from them. \n",
    "\n",
    "BeautifulSoup tagline's: \"You didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\" You can learn more about BeautifulSoup on [its website](https://www.crummy.com/software/BeautifulSoup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell Python explicitly to include this extra functionality. We can do so with the **import** command. Using import will make 100% of the library's functionality accessible to us. In this case, we only want to bring in the set of functionality called BeautifulSoup into our program from the *bs4* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to bring in a couple more Python libraries to support BeautifulSoup:\n",
    "* **requests** allows Python to interact with a web server and fetch HTML documents\n",
    "* **csv** helps Python output data into CSV tables\n",
    "\n",
    "These libraries come pre-installed with Python 3, but we still need to add their functionality to our program using the *import* command*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a variable called *url* and set it to a website to scrape. \n",
    "\n",
    "For today, let's explore the blog of [Bethany Nowviskie](http://nowviskie.org/), Exectuive Director of the Digital Library Federation (DLF) and a prominent voice in Digital Humanities. Nowviskie's blog is shared under a [Creative Commons CC-BY license](http://nowviskie.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://nowviskie.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to scrape our data. This involves a three-part process:\n",
    "\n",
    "* Query the website using the Requests library and store the result into a variable\n",
    "* Extract the text data (in this case, HTML code as text) into an output variable\n",
    "* Generate a BeautifulSoup object based upon the raw HTML text data\n",
    "\n",
    "In Python, each of these steps only takes a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query our URL with requests and store the output into response\n",
    "response = requests.get(url)\n",
    " \n",
    "# Extract all of the text data from our query (in this case, raw HTML code) and store it into data\n",
    "data = response.text\n",
    " \n",
    "# Generate a soup object based on our HTML data that lets us navigate the structure of the website\n",
    "soup = BeautifulSoup(data, 'html5lib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the syntax varies in each step. In step three in particular, note that we have to specify 'html5lib' as a paramter of the BeautifulSoup function. html5lib is a **parser** which means its job is to read raw HTML code and help BeautifulSoup interpret its structure (for instance, what are the HTML tags used, how are they organized, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an object called *soup* which includes the HTML from our site along with a set of additional functionality. We can use this functionality by calling **methods**. Methods are functions that belong to an object, and often manipulate data stored in that object. \n",
    "\n",
    "For instance, let's use the **.prettify() method** to output our HTML code as raw text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a \"pretty\" version of the code the *soup* objects contains (\"pretty\" might be an overstatement.) Now that we've confirmed our data has been successfully fetched and stored in *soup*, it's time to move on to Step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transform our web scraped data (give it structure, identify the important elements, run analyses, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our goal in scraping data from Nowviskie's blog?\n",
    "\n",
    "This step requires that we have some underlying idea of what's important to us and how we intend to use this data. It may help to imagine what we want our output to look like (although we may revise this as our code gets more complicated, as we'll see). Let's imagine our goal in this case is:\n",
    "\n",
    "* Generate a table with the most recent blog posts\n",
    "* For now, we want to capture the *post title*, *post summary text*, and *link to the full blog post*\n",
    "\n",
    "We can imagine this output as a table in .csv format, which includes heading labels, and each post represented as a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First, we need to look at our website and understand how it's organized. To do this, we can use a number of methods, I prefer to use the \"Inspect Element\" feature on either Firefox or Chrome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// **As a group, explore the HTML document and learn how blog posts are structured.** //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have discovered a **tag type** and a **class** for each individual blog post on the website.\n",
    "\n",
    "The **.find_all() method** in our BeautifulSoup object (*soup*) allows us to create a subset of HTML tags and related data that match a search condition. The output of find_all() will be a list of tags that match these conditions. Let's generate a list of blog posts using this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the text below, replace CLASS_TYPE with values for blog posts\n",
    "# Note that you must surround this value with quotations, e.g.\"name_of_class\"\n",
    "\n",
    "articles = soup.find_all('div', class_=\"CLASS_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we've collected in the articles variable by using a print() statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of information! It's a little hard to tell what's going on here. Let's use the len() function to make sure that articles is a list that contains all of the blog posts on the website we just scraped.\n",
    "\n",
    "(Note that this number won't refer to all posts in the entire website, but rather every post that displays on the front page.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the number \"5\", you have successfully stored data about the five blog posts into the articles list.\n",
    "\n",
    "Now that we have our data in a list, we need to extract the specific data we're interested in. If you remember from earlier, we decided we want three data points per post:\n",
    "\n",
    "* Title of the blog post\n",
    "* Post text summary\n",
    "* Link to the full article\n",
    "\n",
    "To do this, we can loop over the articles in the article list. For each article, we can use the **.find(), .get(), and .get_text() BeautifulSoup methods** to extract the information we're interested in. \n",
    "\n",
    "Let's start with a loop that iterates over articles (storing each article information into 'current_article' in each iteration), finds the title tag, and prints the text from the tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_article in articles:\n",
    "    article_title = current_article.find('h1')\n",
    "    article_title_text = article_title.get_text(\" \")\n",
    "    print(article_title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every iteration, the script finds the heading tag containing the article title, extracts the text within that tag, and prints out the extracted text.\n",
    "\n",
    "How would you also print out the text summary for each post? Return to the HTML for the blog and identify the class of the tag containing the text for each post. (Hint: the type is \"div\")\n",
    "\n",
    "Now, modify the articles loop to identify the post text tag, extract the text, and print it to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_article in articles:\n",
    "    article_title = current_article.find('h1')\n",
    "    article_title_text = article_title.get_text(\" \")\n",
    "    print(article_title_text)\n",
    "    \n",
    "    # Fill in the class name for articles here:\n",
    "    \n",
    "    post_summary = current_article.find('div', class_='REPLACEWITHCLASS')\n",
    "    \n",
    "    # How would you extract the text from post_summary? \n",
    "    # Fill in the right part of the statement below with the appropriate method\n",
    "  \n",
    "    post_summary_text = #FILL IN WITH METHOD for post_summary to generate text...\n",
    "    \n",
    "    # The next line removes extra whitespace and also removes non-ascii characters\n",
    "    \n",
    "    post_summary_text = post_summary_text.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the URL for the link to the full article. On the blog website, this link appears if you hover over the article title. \n",
    "\n",
    "Let's grab the URL from the article title tag, which we already have from the first step! However, we need to look for the link tag *within* the title tag, and then extract the URL (as opposed to the text, which in this case would still be the title of the blog post). Where is the URL stored within a link tag in HTML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_article in articles:\n",
    "    article_title = current_article.find('h1')\n",
    "    article_title_text = article_title.get_text()\n",
    "    print(article_title_text)\n",
    "    \n",
    "    ## Copy your code for post_summary_text from above here (3 lines below)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Now, let's use the .find() method to identify the link <a> tag within the article title tag\n",
    "    full_article_link = article_title.find('a')   \n",
    "    \n",
    "    # We need to specifically extract the part of the <a> tag that contains the URL.\n",
    "    # The .get() command lets us get information about a specific attribute in a tag.\n",
    "    # Fill in the appropriate attribute below\n",
    "    \n",
    "    full_article_URL = full_article_link.get('FILL IN WITH ATTRIBUTE THAT CONTAINS URL IN <A> tag')\n",
    "    print(full_article_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a functioning script that fetches HTML from our target URL, identifies the set of blog posts on the page, loops over each one and extracts three data points (title, post summary, and full article URL), and prints to the results to screen during each iteraction. \n",
    "\n",
    "However, printing to a screen is often not our desired end result (another programming truism: print() is for humans. That is, it's for coders to see how their code is doing, not for the output or functionality of a program).\n",
    "\n",
    "With this in mind, let's add in the functionality of writing our output to a .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Output the data into a helpful format (CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, there are several ways to generate output files. A general pattern is using the open() function to build a connection to an output file, and storing that relationship into a variable (sometimes called the **file handle**).\n",
    "\n",
    "Let's open up that relationship and store it in *f*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = csv.writer(open(\"blog_output.csv\", \"w\"), quotechar='\"', escapechar=\"'\")   # Open the output file for writing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've opened up the file handle connection, we want to first write our headers to the file. Fortunately, with Python and the CSV library, this is a two-line process: \n",
    "* create a list of header labels, and \n",
    "* write them to the new .csv file via the *f* handle and the csv library\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Post Title\", \"Summary Text\", \"URL\"]\n",
    "f.writerow(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow a similar pattern from here on out, except with one important exception: in addition to printing to the screen, we will be writing rows of data after completing each iteraction.\n",
    "\n",
    "To accomplish this, copy your code from the end of step 2 below, and include one additional file writing line at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_article in articles:\n",
    "    # Copy code from step 2 here\n",
    "    # There should be code to generate all three variables used by f.writerow() below\n",
    "    \n",
    "    f.writerow([article_title_text, post_summary_text, full_article_URL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your working directory in Azure. You should now see a new file: blog_output.csv! Try downloading this file to your computer and opening in a program like Excel. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step ?: Additional challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully implemented the web scraping pattern: fetch raw data from a website, transform and structure the data, and output it to a .csv file. As you become interested in different types of outputs, you may wish to complexify this basic recipe - especially in the second \"transform and structure' step. \n",
    "\n",
    "Below are some challenge questions, along with a transcript of the functioning code for the recipe above. We'll discuss this a bit in class, but feel free to explore further on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge #1: Introduce pagination (to fetch data from all pages instead of just the home page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the resulting .csv file captures the blog posts on the first page. But what about additional pages? **Pagination** is the web crawling technique for dealing with data that is hidden behind additional pages -- in this case, behind the \"Older Entry\" link.\n",
    "\n",
    "Try clicking on the Older Entires link on the bottom of the page. You may notice the address bar now reads: http://nowviskie.org/page/2/. Indeed by clicking back and back, you will find the page/ structure continues through the final page, http://nowviskie.org/page/15/. Further, you can use http://nowviskie.org/page/2 instead of http://nowviskie.org/page/2/, and also you can use http://nowviskie.org/page/1 to get to the first/home page. \n",
    "\n",
    "What's the significance of this? In our current code, you have used a single for loop to iterate over the articles on the homepage. To complete this challene with all 15 pages, you will introduce an even larger loop that contains, in itself, the first few steps of the code: fetching from a url, constructing a BeautifulSoup object, and fetching all of the articles. You must however also move the file opening and header line writing steps *outside* of this loop, as these things still only happen once.\n",
    "\n",
    "Here's a template to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "f = csv.writer(open(\"blog_output_challenge1.csv\", \"w\"), quotechar= '\"', escapechar=\"'\")   # Open the output file for writing\n",
    "# Write headers\n",
    "headers = [\"Post Title\", \"Summary Text\", \"URL\"]\n",
    "f.writerow(headers)\n",
    "\n",
    "for page_num in range(1,16):\n",
    "    url = \"http://nowviskie.org/page/\" + str(page_num)\n",
    "    # Query our URL with requests and store the output into response\n",
    "    response = requests.get(url)\n",
    "    # Extract all of the text data from our query (in this case, raw HTML code) and store it into data\n",
    "    data = response.text\n",
    "    # Generate a soup object based on our HTML data that lets us navigate the structure of the website\n",
    "    soup = BeautifulSoup(data, 'html5lib')\n",
    "\n",
    "    articles = soup.find_all('div', class_=\"article\")\n",
    "\n",
    "    for current_article in articles:\n",
    "    ## Your remaining code goes here...\n",
    "    ## Remember to keep all of this code indented one tab in to so it belongs to the for page_num loop\n",
    "    ## (or two tabs if it's part of a second loop inside the page_num loop...)\n",
    "        article_title = current_article.find('h1')\n",
    "        article_title_text = article_title.get_text()\n",
    "        print(article_title_text)\n",
    "\n",
    "        post_summary = current_article.find('div', class_=\"content\")\n",
    "        post_summary_text = post_summary.get_text(\" \")\n",
    "        post_summary_text = post_summary_text.encode('ascii', 'ignore')\n",
    "        print(post_summary_text)   \n",
    "\n",
    "\n",
    "        full_article_link = article_title.find('a')   \n",
    "        full_article_URL = full_article_link.get('href')\n",
    "        print(full_article_URL)\n",
    "\n",
    "        f.writerow([article_title_text, post_summary_text, full_article_URL])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge #2: Add a fourth category: summary post character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: the len() function returns the number of items in a list. \n",
    "# Another way to think of a string is a list of alphanumeric characters...\n",
    "\n",
    "## Remmeber, you will need to modify the headers list to include a fourth value \n",
    "# (and also modify the f.writerow() step in the for loop below)\n",
    "\n",
    "headers = [\"Post Title\", \"Summary Text\", \"URL\"]\n",
    "f.writerow(headers)\n",
    "\n",
    "for current_article in articles:\n",
    "    article_title = current_article.find('h1')\n",
    "    article_title_text = article_title.get_text(\" \")\n",
    "    print(article_title_text)\n",
    "    \n",
    "    post_summary = current_article.find('div', class_=\"content\")\n",
    "    post_summary_text = post_summary.get_text(strip=True)\n",
    "    post_summary_text = post_summary_text.strip()\n",
    "    print(post_summary_text)   \n",
    "    \n",
    "\n",
    "    full_article_link = article_title.find('a')   \n",
    "    full_article_URL = full_article_link.get('href')\n",
    "    print(full_article_URL)\n",
    "    \n",
    "    f.writerow([article_title_text, post_summary_text, full_article_URL])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge #3: Add a fifth category: vocabulary complexity (or average word length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will want to expand on your solution to Challenge #1, so we've left the code box below blank\n",
    "# Hint: You may find it important to count the number of words in a string.\n",
    "# To do this, you will need to (1) split a string into a list of words, and (2) count the number of items in that list\n",
    "\n",
    "# For #1, you can call the .split() method on a string.\n",
    "# For instance, \"Here is a sentence\".split() will return [\"Here\", \"is\", \"a\", \"sentence\"]\n",
    "\n",
    "# For #2, you may find the len() function handy once again!!\n",
    "\n",
    "# Remember to modify your output file name to save to a different .csv (along with the header row and f.writewrow() step)\n",
    "# ...\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge #4: Add a sixth category to count the instances of a specfic word of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appendix\n",
    "\n",
    "## Here is a cheat sheet featuring the values for the blanks above, in case you get stuck!\n",
    "\n",
    "# To generate a list of articles:\n",
    "# articles = soup.find_all('div', class_=\"article\")\n",
    "\n",
    "# To print out the post summary text:\n",
    "#     post_summary = current_article.find('div', class_=\"content\")\n",
    "#     post_summary_text = post_summary.get_text(\" \")\n",
    "#     post_summary_text = post_summary_text.strip().encode('ascii', 'ignore')\n",
    "\n",
    "#     print(post_summary_text)\n",
    "\n",
    "# To extract attribute containing URL from a link:\n",
    "#     full_article_URL = full_article_link.get('href')\n",
    "\n",
    "# Challenge 1 solution:\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import csv\n",
    "\n",
    "# f = csv.writer(open(\"blog_output_challenge1.csv\", \"w\"), escapechar='\\\\')   # Open the output file for writing\n",
    "# # Write headers\n",
    "# headers = [\"Post Title\", \"Summary Text\", \"URL\"]\n",
    "# f.writerow(headers)\n",
    "\n",
    "# for page_num in range(1,16):\n",
    "#     url = \"http://nowviskie.org/page/\" + str(page_num)\n",
    "#     # Query our URL with requests and store the output into response\n",
    "#     response = requests.get(url)\n",
    "#     # Extract all of the text data from our query (in this case, raw HTML code) and store it into data\n",
    "#     data = response.text\n",
    "#     # Generate a soup object based on our HTML data that lets us navigate the structure of the website\n",
    "#     soup = BeautifulSoup(data, 'html5lib')\n",
    "\n",
    "#     articles = soup.find_all('div', class_=\"article\")\n",
    "\n",
    "#     for current_article in articles:\n",
    "#         article_title = current_article.find('h1')\n",
    "#         article_title_text = article_title.get_text(\" \")\n",
    "#         print(article_title_text)\n",
    "\n",
    "#         post_summary = current_article.find('div', class_=\"content\")\n",
    "#         post_summary_text = post_summary.get_text()\n",
    "#         post_summary_text = post_summary_text.strip().encode('ascii', 'ignore')\n",
    "#         print(post_summary_text)   \n",
    "\n",
    "\n",
    "#         full_article_link = article_title.find('a')   \n",
    "#         full_article_URL = full_article_link.get('href')\n",
    "#         print(full_article_URL)\n",
    "\n",
    "#         f.writerow([article_title_text, post_summary_text, full_article_URL])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
